{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generic imports\n",
    "import os, sys, glob, tempfile, pickle\n",
    "import numpy as np\n",
    "import scipy.linalg as sl, scipy.optimize as so\n",
    "import matplotlib.pyplot as plt\n",
    "import numdifftools as nd\n",
    "import corner\n",
    "\n",
    "# Non-traditional packages\n",
    "import libstempo as lt\n",
    "\n",
    "# The actual outlier code\n",
    "import interval as itvl\n",
    "from nutstrajectory import nuts6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up the Likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First pick your pulsar and locate the corresponding par and tim files. By default this cell will look in your current working directoy, but it can always be redefined as necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psr = 'B1855+09'\n",
    "parfile = glob.glob(f'./{psr}*.par')[0]\n",
    "timfile = glob.glob(f'./{psr}*.tim')[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This creates an Interval Likelihood object, which will load/process the pulsar data and perform a few coordinate transformations to get it ready for HMC. If you're curious about the output, just uncomment the second line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "likob = itvl.Interval(parfile, timfile)\n",
    "# print(likob.full_loglikelihood_grad(likob.pstart))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to get an approximate maximum of the posterior. First we'll split our log_likelihood_grad function into two parts: one that returns the ll, and one that returns the gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func(x):\n",
    "    ll, _ = likob.full_loglikelihood_grad(x)\n",
    "    \n",
    "    return -np.inf if np.isnan(ll) else ll\n",
    "\n",
    "def jac(x):\n",
    "    _, j = likob.full_loglikelihood_grad(x)\n",
    "    return j"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's the log likelihood for our starting parameter vector?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "func(likob.pstart)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we compute the approximate maximum, and save it to a pickle file if it doesn't already exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpfile = psr + '-endp.pickle'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if not os.path.isfile(endpfile):\n",
    "    endp = likob.pstart\n",
    "    for iter in range(3):\n",
    "        res = so.minimize(lambda x: -func(x),\n",
    "                          endp,\n",
    "                          jac=lambda x: -jac(x),\n",
    "                          hess=None,\n",
    "                          method='L-BFGS-B', options={'disp': True})\n",
    "\n",
    "        endp = res['x']\n",
    "    pickle.dump(endp,open(endpfile,'wb'))\n",
    "else:\n",
    "    endp = pickle.load(open(endpfile,'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our approximate maximum should be larger. You can double-check that here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "func(endp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we want to 'whiten' our likelihood (a reparameterization where our new covariance matrix is the identity matrix). To that we'll need to compute the Hessian of our posterior. This calculation may take some time relative to the maximization function above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nhyperpars = likob.ptadict[likob.pname + '_outlierprob'] + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hessfile = psr + '-fullhessian.pickle'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if not os.path.isfile(hessfile):\n",
    "    reslice = np.arange(0,nhyperpars)\n",
    "\n",
    "    def partfunc(x):\n",
    "        p = np.copy(endp)\n",
    "        p[reslice] = x\n",
    "        return likob.full_loglikelihood_grad(p)[0]\n",
    "\n",
    "    ndhessdiag = nd.Hessdiag(func)\n",
    "    ndparthess = nd.Hessian(partfunc)\n",
    "\n",
    "    # Create a good-enough approximation for the Hessian\n",
    "    nhdiag = ndhessdiag(endp)\n",
    "    nhpart = ndparthess(endp[reslice])\n",
    "    fullhessian = np.diag(nhdiag)\n",
    "    fullhessian[:nhyperpars,:nhyperpars] = nhpart\n",
    "    pickle.dump(fullhessian,open(hessfile,'wb'))\n",
    "else:\n",
    "    fullhessian = pickle.load(open(hessfile,'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the Hessian in hand, we can whiten our likelihood object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wl = itvl.whitenedLikelihood(likob, endp, -fullhessian)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This new likelihood object should have the same value as the old Interval one. As a sanity check, you can run this below to check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "likob.pstart = endp\n",
    "wlps = wl.forward(endp)\n",
    "print(likob.full_loglikelihood_grad(endp))\n",
    "print(wl.likob.full_loglikelihood_grad(wl.backward(wlps)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NUTS Sampler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to sample. Set up the directory where you want the chains to be stored (or let this code make it for you), and define the number of samples your want to run for. The default is 20,000. You can also set the length of the burn-in, which is defaulted to 1,000 samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chaindir = 'chains_' + psr\n",
    "Nsamples = 20000\n",
    "Nburnin = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p {chaindir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "chainfile = chaindir + '/samples.txt'\n",
    "if not os.path.isfile(chainfile) or len(open(chainfile,'r').readlines()) < 19999:\n",
    "    # Run NUTS for 20000 samples, with a burn-in of 1000 samples (target acceptance = 0.6)\n",
    "    samples, lnprob, epsilon = nuts6(wl.loglikelihood_grad, Nsamples, Nburnin,\n",
    "                                     wlps, 0.6,\n",
    "                                     verbose=True,\n",
    "                                     outFile=chainfile,\n",
    "                                     pickleFile=chaindir + '/save')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the sampling complete, we can now analyze the chains and look for outliers in our data. The first step is undo all of our coordinate transformations to get back to real, tangible values of our parameters. Each NUTS sample generated gives one parameter vector, and the full array for all samples is saved to a .npy object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsfile = psr + '-pars.npy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if not os.path.isfile(parsfile):\n",
    "    samples = np.loadtxt(chaindir + '/samples.txt')\n",
    "    fullsamp = wl.backward(samples[:,:-2])\n",
    "    funnelsamp = likob.backward(fullsamp)\n",
    "    pars = likob.multi_full_backward(funnelsamp)\n",
    "    np.save(parsfile,pars)\n",
    "else:\n",
    "    pars = np.load(parsfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's look at a corner plot of the posteriors of all our hyperparameters (including the new outlier parameter)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parnames = list(likob.ptadict.keys())\n",
    "if not os.path.isfile(psr + '-corner.pdf'):\n",
    "    corner.corner(pars[:,:nhyperpars],labels=parnames[:nhyperpars],show_titles=True);\n",
    "    plt.savefig(psr + '-corner.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we finally look for outliers. The NUTS sampler used an outlier-robust likelihood, so our job now is to compute some 'outlier probability' for each observation in the dataset. We will get a vector of these outlier probabilities, one per TOA, and a vector of the corresponding uncertainties in this outlier probability. This calculation uses the pulse period of the pulsar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "likob.P0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def poutlier(p,likob):\n",
    "    \"\"\"Invoked on a sample parameter set and the appropriate likelihood,\n",
    "    returns the outlier probability (a vector over the TOAs) and\n",
    "    the individual sqrt(chisq) values\"\"\"\n",
    "    \n",
    "    # invoke the likelihood\n",
    "    _, _ = likob.base_loglikelihood_grad(p)\n",
    "\n",
    "    # get the piccard pulsar object\n",
    "    # psr = likob.psr\n",
    "\n",
    "    r = likob.detresiduals\n",
    "    N = likob.Nvec\n",
    "\n",
    "    Pb = likob.outlier_prob # a priori outlier probability for this sample\n",
    "    P0 = likob.P0           # width of outlier range\n",
    "    \n",
    "    PA = 1.0 - Pb\n",
    "    PB = Pb\n",
    "    \n",
    "    PtA = np.exp(-0.5*r**2/N) / np.sqrt(2*np.pi*N)\n",
    "    PtB = 1.0/P0\n",
    "    \n",
    "    num = PtB * PB\n",
    "    den = PtB * PB + PtA * PA\n",
    "    \n",
    "    return num/den, r/np.sqrt(N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we compute the vector of outlier probabilities and their uncertainties, and save it to a .npy file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pobsfile = psr + '-pobs.npy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if not os.path.isfile(pobsfile):\n",
    "    nsamples = len(pars)\n",
    "    nobs = len(likob.Nvec)\n",
    "\n",
    "    # basic likelihood\n",
    "    lo = likob\n",
    "\n",
    "    outps = np.zeros((nsamples,nobs),'d')\n",
    "    sigma = np.zeros((nsamples,nobs),'d')\n",
    "\n",
    "    for i,p in enumerate(pars):\n",
    "        outps[i,:], sigma[i,:] = poutlier(p,lo)\n",
    "\n",
    "    out = np.zeros((nsamples,nobs,2),'d')\n",
    "    out[:,:,0], out[:,:,1] = outps, sigma    \n",
    "    np.save(pobsfile,out)\n",
    "else:\n",
    "    out = np.load(pobsfile)\n",
    "    outps, sigma = out[:,:,0], out[:,:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avgps = np.mean(outps,axis=0)\n",
    "medps = np.median(outps,axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we can see where these outliers are (or if there even are any!). This will produce a typical plot of TOA residuals, but TOAs with a 10% median outlier probability will be highlighted. If there are no TOAs that reach this criterion, then any with >0.05% probability will be highlighted. Both of these thresholds can be changed at will."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spd = 86400.0   # seconds per day\n",
    "T0 = 53000.0        # reference MJD that is subtracted off all TOAs when processing through libstempo\n",
    "residualplot = psr + '-residuals.pdf'\n",
    "\n",
    "if not os.path.isfile(residualplot):\n",
    "    outliers = medps > 0.1\n",
    "    nout = np.sum(outliers)\n",
    "    nbig = nout\n",
    "    \n",
    "    print(\"Big: {}\".format(nbig))\n",
    "    \n",
    "    if nout == 0:\n",
    "        outliers = medps > 5e-4\n",
    "        nout = np.sum(outliers)\n",
    "    \n",
    "    print(\"Plotted: {}\".format(nout))\n",
    "\n",
    "    plt.figure(figsize=(15,6))\n",
    "\n",
    "    psrobj = likob.psr\n",
    "\n",
    "    # convert toas to mjds\n",
    "    toas = psrobj.toas/spd + T0\n",
    "\n",
    "    # red noise at the starting fit point\n",
    "    _, _ = likob.full_loglikelihood_grad(endp)\n",
    "    rednoise = psrobj.residuals - likob.detresiduals\n",
    "\n",
    "    # plot tim-file residuals (I think)\n",
    "    plt.errorbar(toas,psrobj.residuals,yerr=psrobj.toaerrs,fmt='.',alpha=0.3)\n",
    "\n",
    "    # red noise\n",
    "    # plt.plot(toas,rednoise,'r-')\n",
    "\n",
    "    # possible outliers\n",
    "    plt.errorbar(toas[outliers],psrobj.residuals[outliers],yerr=psrobj.toaerrs[outliers],fmt='rx')\n",
    "\n",
    "    plt.savefig(residualplot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to know the exact indices of the highlighted TOAs (and their corresponding outlier probabilities), you can run the block below to print them out. This can (and probably should) be rewritten to just save this info to another file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ii, elem in enumerate(outliers):\n",
    "    if elem:\n",
    "        print(f'Outlier TOA index: {ii}')\n",
    "        print(f'Outlier probability: {medps[ii]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
